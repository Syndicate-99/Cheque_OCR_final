{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class ModelConfigs():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_path = os.path.join(\"Models/04_sentence_recognition\", datetime.strftime(datetime.now(), \"%Y%m%d%H%M\"))\n",
    "        self.vocab = \"\"\n",
    "        self.height = 96\n",
    "        self.width = 1408\n",
    "        self.max_text_length = 0\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.0005\n",
    "        self.train_epochs = 1000\n",
    "        self.train_workers = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "import tensorflow as tf\n",
    "\n",
    "class ReshapeLayer(layers.Layer):\n",
    "    def call(self, x):\n",
    "        # Get the shape of the input tensor\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        # Reshape to (batch_size, time_steps, features)\n",
    "        return tf.reshape(x, [batch_size, -1, x.shape[-1]])\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates a CNN + RNN model for text recognition\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of unique characters in vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name='input')\n",
    "    \n",
    "    # CNN Feature Extraction\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool1')(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='conv2')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool2')(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='conv3')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool3')(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv4')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name='pool4')(x)\n",
    "    \n",
    "    # Dense layer for feature processing\n",
    "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='conv5')(x)\n",
    "    \n",
    "    # Reshape for RNN using custom layer\n",
    "    x = ReshapeLayer()(x)\n",
    "    \n",
    "    # RNN layers\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(output_dim + 1, activation='softmax', name='dense2')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='ocr_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 5\n",
    "        self.train_workers = 4\n",
    "        self.model_path = \"Models/cheque_recognition\"\n",
    "        \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to JSON file\"\"\"\n",
    "        import json\n",
    "        import os\n",
    "        \n",
    "        os.makedirs(self.model_path, exist_ok=True)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"configs.json\")\n",
    "        config_dict = {\n",
    "            \"vocab\": self.vocab,\n",
    "            \"max_text_length\": self.max_text_length,\n",
    "            \"height\": self.height,\n",
    "            \"width\": self.width,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"train_epochs\": self.train_epochs,\n",
    "            \"train_workers\": self.train_workers,\n",
    "            \"model_path\": self.model_path\n",
    "        }\n",
    "        \n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 112/112 [00:08<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12\n",
      "Maximum text length: 6\n",
      "Dataset size: 336\n",
      "WARNING:tensorflow:From c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ocr_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ocr_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                   </span>┃<span style=\"font-weight: bold\"> Output Shape                        </span>┃<span style=\"font-weight: bold\">             Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ reshape_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReshapeLayer</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">3,341</span> │\n",
       "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m            Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv1 (\u001b[38;5;33mConv2D\u001b[0m)                                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                 \u001b[38;5;34m896\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2 (\u001b[38;5;33mConv2D\u001b[0m)                                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │              \u001b[38;5;34m18,496\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv3 (\u001b[38;5;33mConv2D\u001b[0m)                                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │              \u001b[38;5;34m73,856\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv4 (\u001b[38;5;33mConv2D\u001b[0m)                                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │             \u001b[38;5;34m295,168\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ pool4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv5 (\u001b[38;5;33mConv2D\u001b[0m)                                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │             \u001b[38;5;34m590,080\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ reshape_layer_4 (\u001b[38;5;33mReshapeLayer\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │           \u001b[38;5;34m1,050,624\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   │             \u001b[38;5;34m656,384\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dense2 (\u001b[38;5;33mDense\u001b[0m)                                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)                    │               \u001b[38;5;34m3,341\u001b[0m │\n",
       "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,688,845</span> (10.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,688,845\u001b[0m (10.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,688,845</span> (10.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,688,845\u001b[0m (10.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - CER: 19.7123 - WER: 1.0000 - loss: 454.5406\n",
      "Epoch 1: val_CER improved from inf to 9.12351, saving model to Models/cheque_recognition/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 3s/step - CER: 18.8336 - WER: 1.0000 - loss: 438.7499 - val_CER: 9.1235 - val_WER: 1.0000 - val_loss: 46.7994 - learning_rate: 0.0010\n",
      "Epoch 2/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - CER: 6.5956 - WER: 1.0000 - loss: 50.9357\n",
      "Epoch 2: val_CER improved from 9.12351 to 5.03249, saving model to Models/cheque_recognition/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 3s/step - CER: 6.4740 - WER: 1.0000 - loss: 50.2338 - val_CER: 5.0325 - val_WER: 1.0000 - val_loss: 47.9402 - learning_rate: 0.0010\n",
      "Epoch 3/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - CER: 4.2419 - WER: 1.0000 - loss: 47.8624\n",
      "Epoch 3: val_CER improved from 5.03249 to 3.68171, saving model to Models/cheque_recognition/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - CER: 4.2002 - WER: 1.0000 - loss: 46.8362 - val_CER: 3.6817 - val_WER: 1.0000 - val_loss: 21.1247 - learning_rate: 0.0010\n",
      "Epoch 4/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - CER: 3.3367 - WER: 1.0000 - loss: 20.0882\n",
      "Epoch 4: val_CER improved from 3.68171 to 3.04105, saving model to Models/cheque_recognition/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3s/step - CER: 3.3151 - WER: 1.0000 - loss: 20.8513 - val_CER: 3.0410 - val_WER: 1.0000 - val_loss: 20.0646 - learning_rate: 0.0010\n",
      "Epoch 5/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - CER: 2.8540 - WER: 1.0000 - loss: 22.6993\n",
      "Epoch 5: val_CER improved from 3.04105 to 2.67609, saving model to Models/cheque_recognition/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 3s/step - CER: 2.8414 - WER: 1.0000 - loss: 22.7873 - val_CER: 2.6761 - val_WER: 1.0000 - val_loss: 15.5059 - learning_rate: 0.0010\n",
      "'Functional' object has no attribute '_get_save_spec'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.preprocessors import ImageReader\n",
    "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding\n",
    "from mltu.augmentors import RandomBrightness, RandomRotate, RandomErodeDilate, RandomSharpen\n",
    "from mltu.annotations.images import CVImage\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
    "from mltu.tensorflow.metrics import CERMetric, WERMetric\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_dataset(image_folder, label_folder, target_size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Load dataset and create a list of [processed_image_path, label] pairs\n",
    "    Returns:\n",
    "        dataset: List of [image_path, label] pairs\n",
    "        vocab: Set of unique characters in labels\n",
    "        max_len: Maximum length of any label\n",
    "    \"\"\"\n",
    "    dataset, vocab, max_len = [], set(), 0\n",
    "    valid_formats = ('.jpg', '.jpeg', '.png', '.tif')\n",
    "    \n",
    "    # Create a directory for processed images if it doesn't exist\n",
    "    processed_dir = os.path.join(os.path.dirname(image_folder), 'processed_images')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    image_files = sorted([f for f in os.listdir(image_folder) \n",
    "                         if f.lower().endswith(valid_formats)],\n",
    "                        reverse=True)\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        # Read the image\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load the corresponding label file\n",
    "        label_file = os.path.splitext(image_file)[0] + \".json\"\n",
    "        label_path = os.path.join(label_folder, label_file)\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Label file for {image_file} not found.\")\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = json.load(f)\n",
    "        \n",
    "        # Process each labeled region\n",
    "        for idx, shape in enumerate(labels['shapes']):\n",
    "            points = shape['points']\n",
    "            label = shape['label']\n",
    "            \n",
    "            # Get bounding box coordinates\n",
    "            x_min, y_min = map(int, points[0])\n",
    "            x_max, y_max = map(int, points[1])\n",
    "            \n",
    "            # Crop the labeled region\n",
    "            crop = img[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            # Calculate padding\n",
    "            crop_h, crop_w = crop.shape[:2]\n",
    "            pad_top = max((target_size[0] - crop_h) // 2, 0)\n",
    "            pad_bottom = max(target_size[0] - crop_h - pad_top, 0)\n",
    "            pad_left = max((target_size[1] - crop_w) // 2, 0)\n",
    "            pad_right = max(target_size[1] - crop_w - pad_left, 0)\n",
    "            \n",
    "            # Apply padding with white pixels\n",
    "            padded_crop = cv2.copyMakeBorder(\n",
    "                crop, pad_top, pad_bottom, pad_left, pad_right,\n",
    "                cv2.BORDER_CONSTANT, value=(255, 255, 255)\n",
    "            )\n",
    "            \n",
    "            # Save processed image\n",
    "            processed_name = f\"{os.path.splitext(image_file)[0]}_{idx}.png\"\n",
    "            processed_path = os.path.join(processed_dir, processed_name)\n",
    "            cv2.imwrite(processed_path, cv2.cvtColor(padded_crop, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Add to dataset\n",
    "            dataset.append([processed_path, label])\n",
    "            vocab.update(list(label))\n",
    "            max_len = max(max_len, len(label))\n",
    "    \n",
    "    return dataset, vocab, max_len\n",
    "\n",
    "# Define paths\n",
    "image_folder = r\"C:\\Users\\Kingstone\\Desktop\\All folder\\project work\\IDRBT_Cheque_Image_Dataset\\code\\train\\data\\image\"\n",
    "label_folder = r\"C:\\Users\\Kingstone\\Desktop\\All folder\\project work\\IDRBT_Cheque_Image_Dataset\\code\\train\\data\\labels\"\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and processing dataset...\")\n",
    "dataset, vocab, max_len = load_dataset(image_folder, label_folder, target_size=(300, 300))\n",
    "\n",
    "# Create a ModelConfigs object to store model configurations\n",
    "configs = ModelConfigs()\n",
    "\n",
    "# Save vocab and maximum text length to configs\n",
    "configs.vocab = \"\".join(sorted(vocab))\n",
    "configs.max_text_length = max_len\n",
    "configs.height = 300  # Match target_size\n",
    "configs.width = 300   # Match target_size\n",
    "configs.save()\n",
    "\n",
    "print(f\"Vocabulary size: {len(configs.vocab)}\")\n",
    "print(f\"Maximum text length: {configs.max_text_length}\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Create a data provider for the dataset\n",
    "data_provider = DataProvider(\n",
    "    dataset=dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height, keep_aspect_ratio=True),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data_provider, val_data_provider = data_provider.split(split=0.9)\n",
    "\n",
    "# Augment training data with random transformations\n",
    "train_data_provider.augmentors = [\n",
    "    RandomBrightness(),\n",
    "    RandomErodeDilate(),\n",
    "    RandomSharpen(),\n",
    "]\n",
    "\n",
    "# Create and compile model\n",
    "model = train_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab),\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "model.summary(line_length=110)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_CER\", patience=20, verbose=1, mode=\"min\"),\n",
    "    ModelCheckpoint(f\"{configs.model_path}/model.keras\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\"),\n",
    "    TrainLogger(configs.model_path),\n",
    "    TensorBoard(f\"{configs.model_path}/logs\", update_freq=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_CER\", factor=0.9, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\"),\n",
    "    Model2onnx(f\"{configs.model_path}/model.keras\")\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=callbacks,\n",
    "    #workers=configs.train_workers\n",
    ")\n",
    "\n",
    "# Save training and validation datasets as csv files\n",
    "train_data_provider.to_csv(os.path.join(configs.model_path, \"train.csv\"))\n",
    "val_data_provider.to_csv(os.path.join(configs.model_path, \"val.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates a ResNet-based OCR model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "    \n",
    "    # Load ResNet50 without top layers, freeze early layers\n",
    "    resnet = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers (up to block3)\n",
    "    for layer in resnet.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom layers on top of ResNet\n",
    "    x = resnet.output\n",
    "    \n",
    "    # Reduce height dimension for OCR\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1), padding='same')(x)\n",
    "    \n",
    "    # Additional convolutional blocks for feature extraction\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Prepare for sequence prediction\n",
    "    x = layers.Conv2D(output_dim, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    x = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = layers.Activation('softmax', name='output')(x)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 100\n",
    "        self.model_path = \"./saved_models\"\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to a file\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'max_text_length': self.max_text_length,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'batch_size': self.batch_size,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'train_epochs': self.train_epochs\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 112/112 [00:06<00:00, 18.49it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataProvider.__init__() got an unexpected keyword argument 'prepare_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 249\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 249\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 124\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Create data providers\u001b[39;00m\n\u001b[0;32m    122\u001b[0m label_converter \u001b[38;5;241m=\u001b[39m LabelConverter(configs\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[1;32m--> 124\u001b[0m data_provider \u001b[38;5;241m=\u001b[39m \u001b[43mDataProvider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_preprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mImageReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCVImage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure RGB output\u001b[39;49;00m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mImageResizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_aspect_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_converter\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepare_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add this parameter\u001b[39;49;00m\n\u001b[0;32m    136\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Split into train and validation sets\u001b[39;00m\n\u001b[0;32m    139\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "File \u001b[1;32mc:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mltu\\tensorflow\\dataProvider.py:7\u001b[0m, in \u001b[0;36mDataProvider.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: DataProvider.__init__() got an unexpected keyword argument 'prepare_batch'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.preprocessors import ImageReader\n",
    "from mltu.transformers import ImageResizer\n",
    "from mltu.augmentors import RandomBrightness, RandomRotate, RandomErodeDilate, RandomSharpen\n",
    "from mltu.annotations.images import CVImage\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates a ResNet-based OCR model\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    input_image = layers.Input(shape=input_dim, name=\"image_input\")\n",
    "    input_labels = layers.Input(shape=(None,), dtype=tf.int32, name=\"label_input\")\n",
    "    \n",
    "    # Load ResNet50 without top layers\n",
    "    resnet = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=input_image,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for layer in resnet.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    x = resnet.output\n",
    "    \n",
    "    # Add custom layers for OCR\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1), padding='same')(x)\n",
    "    \n",
    "    # Prepare for sequence processing\n",
    "    shape = x.shape\n",
    "    x = layers.Reshape((shape[1], shape[2] * shape[3]))(x)\n",
    "    \n",
    "    # RNN layers\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "    \n",
    "    # Output layer\n",
    "    x = layers.Dense(output_dim + 1, activation='softmax')(x)  # +1 for CTC blank\n",
    "    \n",
    "    model = Model(inputs=input_image, outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class LabelConverter:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.char_to_num = tf.keras.layers.StringLookup(\n",
    "            vocabulary=list(vocab), num_oov_indices=0, mask_token=None\n",
    "        )\n",
    "    \n",
    "    def __call__(self, data, label):\n",
    "        \"\"\"Convert text to sequence of numbers\"\"\"\n",
    "        # Convert string to list of characters\n",
    "        chars = tf.strings.unicode_split(label, input_encoding='UTF-8')\n",
    "        # Convert chars to indices\n",
    "        nums = self.char_to_num(chars)\n",
    "        # Cast to int32\n",
    "        return data, tf.cast(nums, tf.int32)\n",
    "\n",
    "def train_step(model, optimizer, batch_images, batch_labels):\n",
    "    \"\"\"Custom training step with CTC loss\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        y_pred = model(batch_images, training=True)\n",
    "        \n",
    "        # Prepare lengths\n",
    "        input_length = tf.ones(tf.shape(batch_labels)[0]) * tf.cast(tf.shape(y_pred)[1], dtype=tf.int32)\n",
    "        label_length = tf.cast(tf.reduce_sum(tf.cast(batch_labels != 0, tf.int32), axis=1), tf.int32)\n",
    "        \n",
    "        # Calculate CTC loss\n",
    "        ctc_loss = tf.keras.backend.ctc_batch_cost(\n",
    "            batch_labels,\n",
    "            y_pred,\n",
    "            input_length,\n",
    "            label_length\n",
    "        )\n",
    "        \n",
    "        # Calculate total loss\n",
    "        total_loss = tf.reduce_mean(ctc_loss)\n",
    "    \n",
    "    # Calculate gradients and update weights\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    image_folder = r\"C:\\Users\\Kingstone\\Desktop\\All folder\\project work\\IDRBT_Cheque_Image_Dataset\\code\\train\\data\\image\"\n",
    "    label_folder = r\"C:\\Users\\Kingstone\\Desktop\\All folder\\project work\\IDRBT_Cheque_Image_Dataset\\code\\train\\data\\labels\"\n",
    "\n",
    "    print(\"Loading and processing dataset...\")\n",
    "    dataset, vocab, max_len = load_dataset(image_folder, label_folder)\n",
    "\n",
    "    # Initialize configurations\n",
    "    configs = ModelConfigs()\n",
    "    configs.vocab = \"␢\" + \"\".join(sorted(set(vocab)))  # Add blank token at start\n",
    "    configs.max_text_length = max_len\n",
    "    configs.batch_size = 8  # Reduced batch size\n",
    "    configs.learning_rate = 0.0001  # Reduced learning rate\n",
    "    configs.save()\n",
    "\n",
    "    # Create data providers\n",
    "    label_converter = LabelConverter(configs.vocab)\n",
    "    \n",
    "    data_provider = DataProvider(\n",
    "        dataset=dataset,\n",
    "        skip_validation=True,\n",
    "        batch_size=configs.batch_size,\n",
    "        data_preprocessors=[\n",
    "            ImageReader(CVImage)  # Ensure RGB output\n",
    "        ],\n",
    "        transformers=[\n",
    "            ImageResizer(configs.width, configs.height, keep_aspect_ratio=True),\n",
    "            label_converter\n",
    "        ],\n",
    "        prepare_batch=True  # Add this parameter\n",
    "    )\n",
    "\n",
    "    # Split into train and validation sets\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    train_dataset = dataset[:train_size]\n",
    "    val_dataset = dataset[train_size:]\n",
    "\n",
    "    train_data_provider = DataProvider(\n",
    "        dataset=train_dataset,\n",
    "        skip_validation=True,\n",
    "        batch_size=configs.batch_size,\n",
    "        data_preprocessors=[\n",
    "            ImageReader(CVImage)\n",
    "        ],\n",
    "        transformers=[\n",
    "            ImageResizer(configs.width, configs.height, keep_aspect_ratio=True),\n",
    "            label_converter\n",
    "        ],\n",
    "        prepare_batch=True\n",
    "    )\n",
    "\n",
    "    val_data_provider = DataProvider(\n",
    "        dataset=val_dataset,\n",
    "        skip_validation=True,\n",
    "        batch_size=configs.batch_size,\n",
    "        data_preprocessors=[\n",
    "            ImageReader(CVImage)\n",
    "        ],\n",
    "        transformers=[\n",
    "            ImageResizer(configs.width, configs.height, keep_aspect_ratio=True),\n",
    "            label_converter\n",
    "        ],\n",
    "        prepare_batch=True\n",
    "    )\n",
    "\n",
    "    train_data_provider.augmentors = [\n",
    "        RandomBrightness(),\n",
    "        RandomErodeDilate(),\n",
    "        RandomSharpen(),\n",
    "    ]\n",
    "\n",
    "    # Create model and optimizer\n",
    "    model = train_model(\n",
    "        input_dim=(configs.height, configs.width, 3),\n",
    "        output_dim=len(configs.vocab)\n",
    "    )\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=configs.learning_rate, clipnorm=1.0)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = configs.train_epochs\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_losses = []\n",
    "        for batch_data in tqdm(train_data_provider, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            # Process batch data\n",
    "            if isinstance(batch_data, (list, tuple)):\n",
    "                batch_images = np.array([item[0] for item in batch_data])\n",
    "                batch_labels = np.array([item[1] for item in batch_data])\n",
    "            else:\n",
    "                print(f\"Unexpected batch data format: {type(batch_data)}\")\n",
    "                continue\n",
    "            \n",
    "            loss = train_step(model, optimizer, batch_images, batch_labels)\n",
    "            train_losses.append(loss)\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        \n",
    "        # Validation\n",
    "        val_losses = []\n",
    "        for batch_data in val_data_provider:\n",
    "            # Process batch data\n",
    "            if isinstance(batch_data, (list, tuple)):\n",
    "                batch_images = np.array([item[0] for item in batch_data])\n",
    "                batch_labels = np.array([item[1] for item in batch_data])\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            y_pred = model(batch_images, training=False)\n",
    "            input_length = tf.ones(len(batch_labels)) * tf.cast(tf.shape(y_pred)[1], dtype=tf.int32)\n",
    "            label_length = tf.cast(tf.reduce_sum(tf.cast(batch_labels != 0, tf.int32), axis=1), tf.int32)\n",
    "            \n",
    "            val_loss = tf.keras.backend.ctc_batch_cost(\n",
    "                batch_labels,\n",
    "                y_pred,\n",
    "                input_length,\n",
    "                label_length\n",
    "            )\n",
    "            val_losses.append(tf.reduce_mean(val_loss))\n",
    "        \n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model.save(os.path.join(configs.model_path, \"best_model.h5\"))\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG 19 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates a VGG19-based OCR model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "    \n",
    "    # Load VGG19 without top layers, freeze early layers\n",
    "    vgg = VGG19(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers (first few convolutional blocks)\n",
    "    for layer in vgg.layers[:15]:  # Freeze up to block4\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom layers on top of VGG19\n",
    "    x = vgg.output\n",
    "    \n",
    "    # Reduce height dimension for OCR while preserving width information\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1), padding='same')(x)\n",
    "    \n",
    "    # Additional convolutional blocks for OCR-specific feature extraction\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Prepare for sequence prediction\n",
    "    x = layers.Conv2D(output_dim, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Convert to sequence by averaging across height\n",
    "    x = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)\n",
    "    \n",
    "    # Output layer with softmax activation for character prediction\n",
    "    outputs = layers.Activation('softmax', name='output')(x)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 100\n",
    "        self.model_path = \"./saved_models\"\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to a file\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'max_text_length': self.max_text_length,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'batch_size': self.batch_size,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'train_epochs': self.train_epochs\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing dataset... vgg 19 code \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 112/112 [00:08<00:00, 13.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 12\n",
      "Maximum text length: 6\n",
      "Dataset size: 336\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 2us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                   </span>┃<span style=\"font-weight: bold\"> Output Shape                        </span>┃<span style=\"font-weight: bold\">             Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                 │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │              <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │             <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ batch_normalization_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ batch_normalization_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ batch_normalization_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">6,156</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                       │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)                       │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m            Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m3\u001b[0m)                 │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m1,792\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m300\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │              \u001b[38;5;34m36,928\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │              \u001b[38;5;34m73,856\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │             \u001b[38;5;34m147,584\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │             \u001b[38;5;34m295,168\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │             \u001b[38;5;34m590,080\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │             \u001b[38;5;34m590,080\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_conv4 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │             \u001b[38;5;34m590,080\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_conv4 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_conv4 (\u001b[38;5;33mConv2D\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ batch_normalization_14 (\u001b[38;5;33mBatchNormalization\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │               \u001b[38;5;34m2,048\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ batch_normalization_15 (\u001b[38;5;33mBatchNormalization\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │               \u001b[38;5;34m2,048\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │           \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ batch_normalization_16 (\u001b[38;5;33mBatchNormalization\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │               \u001b[38;5;34m2,048\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m12\u001b[0m)                    │               \u001b[38;5;34m6,156\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ lambda_5 (\u001b[38;5;33mLambda\u001b[0m)                              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m12\u001b[0m)                       │                   \u001b[38;5;34m0\u001b[0m │\n",
       "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
       "│ output (\u001b[38;5;33mActivation\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m12\u001b[0m)                       │                   \u001b[38;5;34m0\u001b[0m │\n",
       "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">27,116,108</span> (103.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m27,116,108\u001b[0m (103.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,887,692</span> (72.05 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,887,692\u001b[0m (72.05 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,228,416</span> (31.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,228,416\u001b[0m (31.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node compile_loss/CTCLoss defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Kingstone\\AppData\\Local\\Temp\\ipykernel_11912\\2533748322.py\", line 169, in <module>\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 398, in _compute_loss\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 366, in compute_loss\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 618, in __call__\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 659, in call\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mltu\\tensorflow\\losses.py\", line 20, in __call__\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\backend.py\", line 666, in ctc_batch_cost\n\nSaw a non-null label (index >= num_classes - 1) following a null label, batch: 1 num_classes: 12 labels: 0,7,9,11,8,10 labels seen so far: 0,7,9\n\t [[{{node compile_loss/CTCLoss}}]] [Op:__inference_one_step_on_iterator_175655]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 169\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 169\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#workers=configs.train_workers\u001b[39;49;00m\n\u001b[0;32m    175\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Save training and validation datasets as csv files\u001b[39;00m\n\u001b[0;32m    178\u001b[0m train_data_provider\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(configs\u001b[38;5;241m.\u001b[39mmodel_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/CTCLoss defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"C:\\Users\\Kingstone\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Kingstone\\AppData\\Local\\Temp\\ipykernel_11912\\2533748322.py\", line 169, in <module>\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 320, in fit\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 121, in one_step_on_iterator\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 108, in one_step_on_data\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 54, in train_step\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 398, in _compute_loss\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 366, in compute_loss\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 618, in __call__\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\compile_utils.py\", line 659, in call\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mltu\\tensorflow\\losses.py\", line 20, in __call__\n\n  File \"c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\backend.py\", line 666, in ctc_batch_cost\n\nSaw a non-null label (index >= num_classes - 1) following a null label, batch: 1 num_classes: 12 labels: 0,7,9,11,8,10 labels seen so far: 0,7,9\n\t [[{{node compile_loss/CTCLoss}}]] [Op:__inference_one_step_on_iterator_175655]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try: [tf.config.experimental.set_memory_growth(gpu, True) for gpu in tf.config.experimental.list_physical_devices(\"GPU\")]\n",
    "except: pass\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.preprocessors import ImageReader\n",
    "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding\n",
    "from mltu.augmentors import RandomBrightness, RandomRotate, RandomErodeDilate, RandomSharpen\n",
    "from mltu.annotations.images import CVImage\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
    "from mltu.tensorflow.metrics import CERMetric, WERMetric\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_dataset(image_folder, label_folder, target_size=(300, 300)):\n",
    "    \"\"\"\n",
    "    Load dataset and create a list of [processed_image_path, label] pairs\n",
    "    Returns:\n",
    "        dataset: List of [image_path, label] pairs\n",
    "        vocab: Set of unique characters in labels\n",
    "        max_len: Maximum length of any label\n",
    "    \"\"\"\n",
    "    dataset, vocab, max_len = [], set(), 0\n",
    "    valid_formats = ('.jpg', '.jpeg', '.png', '.tif')\n",
    "    \n",
    "    # Create a directory for processed images if it doesn't exist\n",
    "    processed_dir = os.path.join(os.path.dirname(image_folder), 'processed_images')\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "    \n",
    "    image_files = sorted([f for f in os.listdir(image_folder) \n",
    "                         if f.lower().endswith(valid_formats)],\n",
    "                        reverse=True)\n",
    "    \n",
    "    for image_file in tqdm(image_files, desc=\"Processing images\"):\n",
    "        # Read the image\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load the corresponding label file\n",
    "        label_file = os.path.splitext(image_file)[0] + \".json\"\n",
    "        label_path = os.path.join(label_folder, label_file)\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Label file for {image_file} not found.\")\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = json.load(f)\n",
    "        \n",
    "        # Process each labeled region\n",
    "        for idx, shape in enumerate(labels['shapes']):\n",
    "            points = shape['points']\n",
    "            label = shape['label']\n",
    "            \n",
    "            # Get bounding box coordinates\n",
    "            x_min, y_min = map(int, points[0])\n",
    "            x_max, y_max = map(int, points[1])\n",
    "            \n",
    "            # Crop the labeled region\n",
    "            crop = img[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            # Calculate padding\n",
    "            crop_h, crop_w = crop.shape[:2]\n",
    "            pad_top = max((target_size[0] - crop_h) // 2, 0)\n",
    "            pad_bottom = max(target_size[0] - crop_h - pad_top, 0)\n",
    "            pad_left = max((target_size[1] - crop_w) // 2, 0)\n",
    "            pad_right = max(target_size[1] - crop_w - pad_left, 0)\n",
    "            \n",
    "            # Apply padding with white pixels\n",
    "            padded_crop = cv2.copyMakeBorder(\n",
    "                crop, pad_top, pad_bottom, pad_left, pad_right,\n",
    "                cv2.BORDER_CONSTANT, value=(255, 255, 255)\n",
    "            )\n",
    "            \n",
    "            # Save processed image\n",
    "            processed_name = f\"{os.path.splitext(image_file)[0]}_{idx}.png\"\n",
    "            processed_path = os.path.join(processed_dir, processed_name)\n",
    "            cv2.imwrite(processed_path, cv2.cvtColor(padded_crop, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Add to dataset\n",
    "            dataset.append([processed_path, label])\n",
    "            vocab.update(list(label))\n",
    "            max_len = max(max_len, len(label))\n",
    "    \n",
    "    return dataset, vocab, max_len\n",
    "\n",
    "# Define paths\n",
    "image_folder = r\"C:\\Users\\Kingstone\\Desktop\\All folder\\project work\\IDRBT_Cheque_Image_Dataset\\code\\train\\data\\image\"\n",
    "label_folder = r\"C:\\Users\\Kingstone\\Desktop\\All folder\\project work\\IDRBT_Cheque_Image_Dataset\\code\\train\\data\\labels\"\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and processing dataset... vgg 19 code \")\n",
    "dataset, vocab, max_len = load_dataset(image_folder, label_folder, target_size=(300, 300))\n",
    "\n",
    "# Create a ModelConfigs object to store model configurations\n",
    "configs = ModelConfigs()\n",
    "\n",
    "# Save vocab and maximum text length to configs\n",
    "configs.vocab = \"\".join(sorted(vocab))\n",
    "configs.max_text_length = max_len\n",
    "configs.height = 300  # Match target_size\n",
    "configs.width = 300   # Match target_size\n",
    "configs.save()\n",
    "\n",
    "print(f\"Vocabulary size: {len(configs.vocab)}\")\n",
    "print(f\"Maximum text length: {configs.max_text_length}\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Create a data provider for the dataset\n",
    "data_provider = DataProvider(\n",
    "    dataset=dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height, keep_aspect_ratio=True),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data_provider, val_data_provider = data_provider.split(split=0.9)\n",
    "\n",
    "# Augment training data with random transformations\n",
    "train_data_provider.augmentors = [\n",
    "    RandomBrightness(),\n",
    "    RandomErodeDilate(),\n",
    "    RandomSharpen(),\n",
    "]\n",
    "\n",
    "# Create and compile model\n",
    "model = train_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab),\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "model.summary(line_length=110)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_CER\", patience=20, verbose=1, mode=\"min\"),\n",
    "    ModelCheckpoint(f\"{configs.model_path}/model.keras\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\"),\n",
    "    TrainLogger(configs.model_path),\n",
    "    TensorBoard(f\"{configs.model_path}/logs\", update_freq=1),\n",
    "    ReduceLROnPlateau(monitor=\"val_CER\", factor=0.9, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\"),\n",
    "    Model2onnx(f\"{configs.model_path}/model.keras\")\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=callbacks,\n",
    "    #workers=configs.train_workers\n",
    ")\n",
    "\n",
    "# Save training and validation datasets as csv files\n",
    "train_data_provider.to_csv(os.path.join(configs.model_path, \"train.csv\"))\n",
    "val_data_provider.to_csv(os.path.join(configs.model_path, \"val.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inception v3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates an InceptionV3-based OCR model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "    \n",
    "    # Load InceptionV3 without top layers, freeze early layers\n",
    "    inception = InceptionV3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers (up to mixed5)\n",
    "    for layer in inception.layers[:249]:  # Freeze up to mixed5\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom layers on top of InceptionV3\n",
    "    x = inception.output\n",
    "    \n",
    "    # Add inception-style module for OCR\n",
    "    tower_1 = layers.Conv2D(192, (1, 1), padding='same', activation='relu')(x)\n",
    "    tower_1 = layers.Conv2D(256, (3, 3), padding='same', activation='relu')(tower_1)\n",
    "    \n",
    "    tower_2 = layers.Conv2D(192, (1, 1), padding='same', activation='relu')(x)\n",
    "    tower_2 = layers.Conv2D(256, (1, 5), padding='same', activation='relu')(tower_2)\n",
    "    tower_2 = layers.Conv2D(256, (5, 1), padding='same', activation='relu')(tower_2)\n",
    "    \n",
    "    tower_3 = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n",
    "    tower_3 = layers.Conv2D(128, (1, 1), padding='same', activation='relu')(tower_3)\n",
    "    \n",
    "    # Concatenate all towers\n",
    "    x = layers.concatenate([tower_1, tower_2, tower_3], axis=3)\n",
    "    \n",
    "    # Reduce height dimension for OCR while preserving width\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1), padding='same')(x)\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Additional convolution for feature refinement\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Prepare for sequence prediction\n",
    "    x = layers.Conv2D(output_dim, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Convert to sequence by averaging across height\n",
    "    x = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = layers.Activation('softmax', name='output')(x)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 100\n",
    "        self.model_path = \"./saved_models\"\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to a file\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'max_text_length': self.max_text_length,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'batch_size': self.batch_size,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'train_epochs': self.train_epochs\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mobileVnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates a MobileNetV2-based OCR model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "    \n",
    "    # Load MobileNetV2 without top layers\n",
    "    mobilenet = MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers (up to block 10)\n",
    "    for layer in mobilenet.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom layers on top of MobileNet\n",
    "    x = mobilenet.output\n",
    "    \n",
    "    # Add depthwise separable convolutions for efficiency\n",
    "    x = layers.SeparableConv2D(512, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Reduce height dimension for OCR while preserving width\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1), padding='same')(x)\n",
    "    \n",
    "    # Additional depthwise separable convolutions\n",
    "    x = layers.SeparableConv2D(512, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Final feature extraction\n",
    "    x = layers.SeparableConv2D(512, (3, 3), padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    # Prepare for sequence prediction\n",
    "    x = layers.Conv2D(output_dim, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Convert to sequence by averaging across height\n",
    "    x = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = layers.Activation('softmax', name='output')(x)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 100\n",
    "        self.model_path = \"./saved_models\"\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to a file\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'max_text_length': self.max_text_length,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'batch_size': self.batch_size,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'train_epochs': self.train_epochs\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates a DenseNet-based OCR model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "    \n",
    "    # Load DenseNet121 without top layers\n",
    "    densenet = DenseNet121(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers (up to block3)\n",
    "    for layer in densenet.layers[:200]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the base model output\n",
    "    x = densenet.output\n",
    "    \n",
    "    # Create a custom dense block for OCR\n",
    "    def dense_block(x, growth_rate, num_layers):\n",
    "        concatenated_inputs = [x]\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            # Composite function: BN -> ReLU -> Conv\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Activation('relu')(x)\n",
    "            x = layers.Conv2D(growth_rate, (3, 3), padding='same')(x)\n",
    "            \n",
    "            # Concatenate with previous layers\n",
    "            concatenated_inputs.append(x)\n",
    "            x = layers.Concatenate(axis=-1)(concatenated_inputs)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Add custom dense block\n",
    "    x = dense_block(x, growth_rate=32, num_layers=4)\n",
    "    \n",
    "    # Transition layer with height reduction for OCR\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(512, (1, 1), padding='same')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1), padding='same')(x)\n",
    "    \n",
    "    # Additional dense block\n",
    "    x = dense_block(x, growth_rate=32, num_layers=2)\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Final convolution layer\n",
    "    x = layers.Conv2D(512, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Prepare for sequence prediction\n",
    "    x = layers.Conv2D(output_dim, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Convert to sequence by averaging across height\n",
    "    x = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = layers.Activation('softmax', name='output')(x)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 100\n",
    "        self.model_path = \"./saved_models\"\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to a file\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'max_text_length': self.max_text_length,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'batch_size': self.batch_size,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'train_epochs': self.train_epochs\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates an Xception-based OCR model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "    \n",
    "    # Load Xception without top layers\n",
    "    xception = Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers (entry flow and middle flow)\n",
    "    for layer in xception.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the base model output\n",
    "    x = xception.output\n",
    "    \n",
    "    # Custom exit flow for OCR\n",
    "    def separable_conv_block(x, filters):\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SeparableConv2D(filters, (3, 3), padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        return x\n",
    "    \n",
    "    # First exit block\n",
    "    residual = layers.Conv2D(512, (1, 1), strides=(2, 1), padding='same')(x)\n",
    "    residual = layers.BatchNormalization()(residual)\n",
    "    \n",
    "    x = separable_conv_block(x, 512)\n",
    "    x = separable_conv_block(x, 512)\n",
    "    x = layers.MaxPooling2D((2, 1), padding='same')(x)\n",
    "    \n",
    "    # Add residual connection\n",
    "    x = layers.add([x, residual])\n",
    "    \n",
    "    # Second exit block with height preservation\n",
    "    x = separable_conv_block(x, 512)\n",
    "    x = separable_conv_block(x, 512)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Additional separable convolutions for feature refinement\n",
    "    x = separable_conv_block(x, 512)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Prepare for sequence prediction\n",
    "    x = layers.Conv2D(output_dim, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Convert to sequence by averaging across height\n",
    "    x = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = layers.Activation('softmax', name='output')(x)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 100\n",
    "        self.model_path = \"./saved_models\"\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to a file\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'max_text_length': self.max_text_length,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'batch_size': self.batch_size,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'train_epochs': self.train_epochs\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "def train_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Creates an EfficientNet-based OCR model\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model ready for training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "    \n",
    "    # Load EfficientNetB0 without top layers\n",
    "    efficient_net = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for layer in efficient_net.layers[:150]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the base model output\n",
    "    x = efficient_net.output\n",
    "    \n",
    "    # Define MBConv block (Mobile Inverted Bottleneck Conv)\n",
    "    def mbconv_block(x, expand_ratio, output_channels, kernel_size=3):\n",
    "        # Expansion phase\n",
    "        channels = x.shape[-1]\n",
    "        x = layers.Conv2D(channels * expand_ratio, 1, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('swish')(x)\n",
    "        \n",
    "        # Depthwise Convolution\n",
    "        x = layers.DepthwiseConv2D(kernel_size, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('swish')(x)\n",
    "        \n",
    "        # Squeeze and Excitation\n",
    "        se = layers.GlobalAveragePooling2D()(x)\n",
    "        se = layers.Dense(channels * expand_ratio // 4, activation='swish')(se)\n",
    "        se = layers.Dense(channels * expand_ratio, activation='sigmoid')(se)\n",
    "        se = layers.Reshape((1, 1, channels * expand_ratio))(se)\n",
    "        x = layers.multiply([x, se])\n",
    "        \n",
    "        # Output phase\n",
    "        x = layers.Conv2D(output_channels, 1, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Add custom OCR-specific layers\n",
    "    # First block with height reduction\n",
    "    x = mbconv_block(x, expand_ratio=4, output_channels=512)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 1), padding='same')(x)\n",
    "    \n",
    "    # Second block\n",
    "    x = mbconv_block(x, expand_ratio=4, output_channels=512)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    # Additional feature refinement\n",
    "    x = layers.Conv2D(512, (1, 1), use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    \n",
    "    # Prepare for sequence prediction\n",
    "    x = layers.Conv2D(output_dim, (1, 1), activation='relu')(x)\n",
    "    \n",
    "    # Convert to sequence by averaging across height\n",
    "    x = layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = layers.Activation('softmax', name='output')(x)\n",
    "    \n",
    "    # Create and return model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "class ModelConfigs:\n",
    "    \"\"\"Configuration class to store model parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab = \"\"\n",
    "        self.max_text_length = 0\n",
    "        self.height = 300\n",
    "        self.width = 300\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        self.train_epochs = 100\n",
    "        self.model_path = \"./saved_models\"\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Save configurations to a file\"\"\"\n",
    "        if not os.path.exists(self.model_path):\n",
    "            os.makedirs(self.model_path)\n",
    "        \n",
    "        config_path = os.path.join(self.model_path, \"config.json\")\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.vocab,\n",
    "                'max_text_length': self.max_text_length,\n",
    "                'height': self.height,\n",
    "                'width': self.width,\n",
    "                'batch_size': self.batch_size,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'train_epochs': self.train_epochs\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# god owns code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - CER: 1.1274 - WER: 1.0000 - loss: 14.9145 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_CER improved from inf to 1.03671, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 13s/step - CER: 1.1198 - WER: 1.0000 - loss: 14.7219 - val_CER: 1.0367 - val_WER: 1.0000 - val_loss: 14.2884 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - CER: 1.0150 - WER: 1.0000 - loss: 9.4917 \n",
      "Epoch 2: val_CER improved from 1.03671 to 0.98896, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 13s/step - CER: 1.0131 - WER: 1.0000 - loss: 9.4285 - val_CER: 0.9890 - val_WER: 1.0000 - val_loss: 10.7846 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - CER: 0.9550 - WER: 0.9981 - loss: 7.1105 \n",
      "Epoch 3: val_CER improved from 0.98896 to 0.90766, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 12s/step - CER: 0.9510 - WER: 0.9977 - loss: 7.0707 - val_CER: 0.9077 - val_WER: 0.9940 - val_loss: 8.3709 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12s/step - CER: 0.8374 - WER: 0.9689 - loss: 5.1043 \n",
      "Epoch 4: val_CER improved from 0.90766 to 0.77840, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 12s/step - CER: 0.8323 - WER: 0.9662 - loss: 5.0586 - val_CER: 0.7784 - val_WER: 0.9412 - val_loss: 7.1198 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - CER: 0.7097 - WER: 0.8811 - loss: 3.2352 \n",
      "Epoch 5: val_CER improved from 0.77840 to 0.65883, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 11s/step - CER: 0.7055 - WER: 0.8771 - loss: 3.2223 - val_CER: 0.6588 - val_WER: 0.8339 - val_loss: 6.0472 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - CER: 0.6085 - WER: 0.7779 - loss: 2.5185 \n",
      "Epoch 6: val_CER improved from 0.65883 to 0.56791, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 11s/step - CER: 0.6053 - WER: 0.7740 - loss: 2.5019 - val_CER: 0.5679 - val_WER: 0.7272 - val_loss: 4.8041 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - CER: 0.5299 - WER: 0.6803 - loss: 1.7869 \n",
      "Epoch 7: val_CER improved from 0.56791 to 0.49876, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 11s/step - CER: 0.5273 - WER: 0.6770 - loss: 1.7723 - val_CER: 0.4988 - val_WER: 0.6395 - val_loss: 4.7465 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - CER: 0.4703 - WER: 0.6031 - loss: 1.0632 \n",
      "Epoch 8: val_CER improved from 0.49876 to 0.44475, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 12s/step - CER: 0.4681 - WER: 0.6003 - loss: 1.0742 - val_CER: 0.4448 - val_WER: 0.5692 - val_loss: 4.1355 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13s/step - CER: 0.4207 - WER: 0.5389 - loss: 0.6589 \n",
      "Epoch 9: val_CER improved from 0.44475 to 0.40010, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 13s/step - CER: 0.4190 - WER: 0.5367 - loss: 0.6618 - val_CER: 0.4001 - val_WER: 0.5119 - val_loss: 4.4912 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11s/step - CER: 0.3791 - WER: 0.4850 - loss: 0.4689 \n",
      "Epoch 10: val_CER improved from 0.40010 to 0.36267, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 12s/step - CER: 0.3777 - WER: 0.4833 - loss: 0.4677 - val_CER: 0.3627 - val_WER: 0.4634 - val_loss: 3.1133 - learning_rate: 1.0000e-04\n",
      "'Functional' object has no attribute '_get_save_spec'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2052bfefb90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Reshape, Conv2D, BatchNormalization, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_resnet_ocr_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Create an OCR model using ResNet50 as the backbone\n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    # Load ResNet50 without top layers\n",
    "    base_model = ResNet50(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_dim\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the output from ResNet50\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Add custom layers for OCR\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Reshape for sequential processing\n",
    "    _, h, w, c = x.shape\n",
    "    x = Reshape((-1, h * c))(x)\n",
    "    \n",
    "    # Add bidirectional LSTM layers\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = Dense(output_dim + 1, activation='softmax')(x)  # +1 for CTC blank label\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace the original train_model function call with this:\n",
    "model = create_resnet_ocr_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab)\n",
    ")\n",
    "\n",
    "# Fine-tuning setup (add this after initial training)\n",
    "def unfreeze_model(model):\n",
    "    \"\"\"\n",
    "    Unfreeze ResNet50 layers for fine-tuning\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# After initial training, you can fine-tune the model:\n",
    "\n",
    "# Fine-tuning (uncomment and use after initial training)\n",
    "model = unfreeze_model(model)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate * 0.1),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Train for additional epochs with unfrozen layers\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27s/step - CER: 1.0814 - WER: 1.0000 - loss: 14.7527 \n",
      "Epoch 1: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 28s/step - CER: 1.0756 - WER: 1.0000 - loss: 14.5900 - val_CER: 1.0141 - val_WER: 1.0000 - val_loss: 11.3462 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27s/step - CER: 1.0052 - WER: 1.0000 - loss: 9.1471 \n",
      "Epoch 2: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 28s/step - CER: 1.0044 - WER: 1.0000 - loss: 9.0957 - val_CER: 0.9893 - val_WER: 1.0000 - val_loss: 9.4465 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26s/step - CER: 0.9555 - WER: 0.9993 - loss: 7.2424 \n",
      "Epoch 3: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 27s/step - CER: 0.9522 - WER: 0.9991 - loss: 7.2245 - val_CER: 0.9182 - val_WER: 0.9980 - val_loss: 7.6941 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25s/step - CER: 0.8736 - WER: 0.9938 - loss: 5.9821 \n",
      "Epoch 4: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 26s/step - CER: 0.8703 - WER: 0.9933 - loss: 5.9681 - val_CER: 0.8289 - val_WER: 0.9799 - val_loss: 5.9110 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26s/step - CER: 0.7799 - WER: 0.9647 - loss: 4.7442 \n",
      "Epoch 5: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 27s/step - CER: 0.7768 - WER: 0.9632 - loss: 4.7440 - val_CER: 0.7408 - val_WER: 0.9399 - val_loss: 5.9797 - learning_rate: 1.0000e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28s/step - CER: 0.6987 - WER: 0.9078 - loss: 3.6664 \n",
      "Epoch 6: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 29s/step - CER: 0.6954 - WER: 0.9046 - loss: 3.6547 - val_CER: 0.6560 - val_WER: 0.8611 - val_loss: 4.3140 - learning_rate: 1.0000e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29s/step - CER: 0.6136 - WER: 0.8086 - loss: 2.7928 \n",
      "Epoch 7: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 30s/step - CER: 0.6109 - WER: 0.8052 - loss: 2.7855 - val_CER: 0.5785 - val_WER: 0.7632 - val_loss: 2.8151 - learning_rate: 1.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29s/step - CER: 0.5440 - WER: 0.7188 - loss: 1.6691 \n",
      "Epoch 8: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 30s/step - CER: 0.5413 - WER: 0.7153 - loss: 1.6479 - val_CER: 0.5091 - val_WER: 0.6737 - val_loss: 1.8980 - learning_rate: 1.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26s/step - CER: 0.4795 - WER: 0.6345 - loss: 1.0530 \n",
      "Epoch 9: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 27s/step - CER: 0.4776 - WER: 0.6320 - loss: 1.0372 - val_CER: 0.4541 - val_WER: 0.6005 - val_loss: 2.3639 - learning_rate: 1.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27s/step - CER: 0.4312 - WER: 0.5707 - loss: 0.6686 \n",
      "Epoch 10: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 27s/step - CER: 0.4297 - WER: 0.5687 - loss: 0.6825 - val_CER: 0.4139 - val_WER: 0.5455 - val_loss: 7.1575 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 102 variables whereas the saved optimizer has 462 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A total of 20 objects could not be loaded. Example error message for object <Conv2D name=block1_conv1, built=True>:\n\nThe shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(3, 3, 3, 64), Received: value.shape=(7, 7, 3, 64). Target variable: <KerasVariable shape=(3, 3, 3, 64), dtype=float32, path=block1_conv1/kernel>\n\nList of objects that could not be loaded:\n[<Conv2D name=block1_conv1, built=True>, <Conv2D name=block1_conv2, built=True>, <Conv2D name=block2_conv1, built=True>, <Conv2D name=block2_conv2, built=True>, <Conv2D name=block3_conv1, built=True>, <Conv2D name=block3_conv2, built=True>, <Conv2D name=block3_conv3, built=True>, <Conv2D name=block3_conv4, built=True>, <Conv2D name=block4_conv1, built=True>, <Conv2D name=block4_conv2, built=True>, <Conv2D name=block4_conv3, built=True>, <Conv2D name=block4_conv4, built=True>, <Conv2D name=block5_conv1, built=True>, <Conv2D name=block5_conv2, built=True>, <Conv2D name=block5_conv3, built=True>, <Conv2D name=block5_conv4, built=True>, <Conv2D name=conv2d_47, built=True>, <BatchNormalization name=batch_normalization_38, built=True>, <LSTMCell name=lstm_cell, built=True>, <LSTMCell name=lstm_cell, built=True>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 82\u001b[0m\n\u001b[0;32m     71\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     72\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m),\n\u001b[0;32m     73\u001b[0m     loss\u001b[38;5;241m=\u001b[39mCTCloss(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Train for additional epochs with unfrozen layers\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_data_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mltu\\tensorflow\\callbacks.py:75\u001b[0m, in \u001b[0;36mModel2onnx.on_train_end\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Converts the model to onnx format after training is finished. \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel2onnx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, onnx_model_path)\n",
      "\u001b[1;31mValueError\u001b[0m: A total of 20 objects could not be loaded. Example error message for object <Conv2D name=block1_conv1, built=True>:\n\nThe shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(3, 3, 3, 64), Received: value.shape=(7, 7, 3, 64). Target variable: <KerasVariable shape=(3, 3, 3, 64), dtype=float32, path=block1_conv1/kernel>\n\nList of objects that could not be loaded:\n[<Conv2D name=block1_conv1, built=True>, <Conv2D name=block1_conv2, built=True>, <Conv2D name=block2_conv1, built=True>, <Conv2D name=block2_conv2, built=True>, <Conv2D name=block3_conv1, built=True>, <Conv2D name=block3_conv2, built=True>, <Conv2D name=block3_conv3, built=True>, <Conv2D name=block3_conv4, built=True>, <Conv2D name=block4_conv1, built=True>, <Conv2D name=block4_conv2, built=True>, <Conv2D name=block4_conv3, built=True>, <Conv2D name=block4_conv4, built=True>, <Conv2D name=block5_conv1, built=True>, <Conv2D name=block5_conv2, built=True>, <Conv2D name=block5_conv3, built=True>, <Conv2D name=block5_conv4, built=True>, <Conv2D name=conv2d_47, built=True>, <BatchNormalization name=batch_normalization_38, built=True>, <LSTMCell name=lstm_cell, built=True>, <LSTMCell name=lstm_cell, built=True>]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Reshape, Conv2D, BatchNormalization, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_resnet_ocr_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Create an OCR model using ResNet50 as the backbone\n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    # Load ResNet50 without top layers\n",
    "    base_model = VGG19(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_dim\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the output from ResNet50\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Add custom layers for OCR\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Reshape for sequential processing\n",
    "    _, h, w, c = x.shape\n",
    "    x = Reshape((-1, h * c))(x)\n",
    "    \n",
    "    # Add bidirectional LSTM layers\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = Dense(output_dim + 1, activation='softmax')(x)  # +1 for CTC blank label\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace the original train_model function call with this:\n",
    "model = create_resnet_ocr_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab)\n",
    ")\n",
    "\n",
    "# Fine-tuning setup (add this after initial training)\n",
    "def unfreeze_model(model):\n",
    "    \"\"\"\n",
    "    Unfreeze ResNet50 layers for fine-tuning\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# After initial training, you can fine-tune the model:\n",
    "\n",
    "# Fine-tuning (uncomment and use after initial training)\n",
    "model = unfreeze_model(model)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate * 0.1),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Train for additional epochs with unfrozen layers\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kingstone\\AppData\\Local\\Temp\\ipykernel_11912\\513621530.py:16: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1us/step\n",
      "Epoch 1/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 1.0869 - WER: 0.9973 - loss: 16.2915\n",
      "Epoch 1: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 5s/step - CER: 1.0800 - WER: 0.9973 - loss: 16.1609 - val_CER: 0.9943 - val_WER: 0.9970 - val_loss: 16.1418 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.9961 - WER: 0.9980 - loss: 10.6635\n",
      "Epoch 2: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 5s/step - CER: 0.9961 - WER: 0.9980 - loss: 10.6233 - val_CER: 0.9872 - val_WER: 0.9985 - val_loss: 14.5555 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.9739 - WER: 0.9988 - loss: 8.6794\n",
      "Epoch 3: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 5s/step - CER: 0.9723 - WER: 0.9988 - loss: 8.6323 - val_CER: 0.9485 - val_WER: 0.9990 - val_loss: 12.3185 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.9207 - WER: 0.9991 - loss: 6.7125\n",
      "Epoch 4: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.9184 - WER: 0.9992 - loss: 6.6923 - val_CER: 0.8901 - val_WER: 0.9985 - val_loss: 10.4111 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.8467 - WER: 0.9979 - loss: 5.2343\n",
      "Epoch 5: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.8437 - WER: 0.9977 - loss: 5.2169 - val_CER: 0.8092 - val_WER: 0.9940 - val_loss: 8.7295 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.7657 - WER: 0.9725 - loss: 4.1013\n",
      "Epoch 6: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.7623 - WER: 0.9705 - loss: 4.0682 - val_CER: 0.7264 - val_WER: 0.9489 - val_loss: 7.8591 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.6857 - WER: 0.9116 - loss: 3.1572\n",
      "Epoch 7: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.6829 - WER: 0.9087 - loss: 3.1242 - val_CER: 0.6531 - val_WER: 0.8793 - val_loss: 7.5900 - learning_rate: 1.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.6182 - WER: 0.8369 - loss: 2.3878\n",
      "Epoch 8: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.6158 - WER: 0.8339 - loss: 2.3701 - val_CER: 0.5914 - val_WER: 0.8025 - val_loss: 8.4397 - learning_rate: 1.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.5613 - WER: 0.7628 - loss: 1.5507\n",
      "Epoch 9: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.5591 - WER: 0.7598 - loss: 1.5538 - val_CER: 0.5373 - val_WER: 0.7288 - val_loss: 9.8350 - learning_rate: 1.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.5137 - WER: 0.6967 - loss: 1.2911\n",
      "Epoch 10: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.5119 - WER: 0.6943 - loss: 1.2870 - val_CER: 0.4946 - val_WER: 0.6696 - val_loss: 8.3260 - learning_rate: 1.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.4729 - WER: 0.6399 - loss: 1.1249\n",
      "Epoch 11: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.4715 - WER: 0.6380 - loss: 1.1266 - val_CER: 0.4584 - val_WER: 0.6190 - val_loss: 9.3128 - learning_rate: 1.0000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.4394 - WER: 0.5936 - loss: 0.7243\n",
      "Epoch 12: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.4382 - WER: 0.5919 - loss: 0.7333 - val_CER: 0.4264 - val_WER: 0.5744 - val_loss: 10.5861 - learning_rate: 1.0000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.4124 - WER: 0.5552 - loss: 1.3001\n",
      "Epoch 13: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.4113 - WER: 0.5537 - loss: 1.2849 - val_CER: 0.4013 - val_WER: 0.5389 - val_loss: 8.5613 - learning_rate: 1.0000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.3879 - WER: 0.5214 - loss: 0.6376\n",
      "Epoch 14: val_CER did not improve from 0.36267\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.3868 - WER: 0.5201 - loss: 0.6290 - val_CER: 0.3771 - val_WER: 0.5066 - val_loss: 9.8159 - learning_rate: 1.0000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.3652 - WER: 0.4903 - loss: 0.7165\n",
      "Epoch 15: val_CER improved from 0.36267 to 0.35618, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.3644 - WER: 0.4891 - loss: 0.7052 - val_CER: 0.3562 - val_WER: 0.4774 - val_loss: 9.1207 - learning_rate: 1.0000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.3451 - WER: 0.4626 - loss: 0.4078\n",
      "Epoch 16: val_CER improved from 0.35618 to 0.33789, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.3443 - WER: 0.4616 - loss: 0.4085 - val_CER: 0.3379 - val_WER: 0.4522 - val_loss: 10.9327 - learning_rate: 1.0000e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.3288 - WER: 0.4403 - loss: 0.6298\n",
      "Epoch 17: val_CER improved from 0.33789 to 0.32222, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.3281 - WER: 0.4394 - loss: 0.6274 - val_CER: 0.3222 - val_WER: 0.4308 - val_loss: 11.2809 - learning_rate: 1.0000e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.3134 - WER: 0.4188 - loss: 0.4469\n",
      "Epoch 18: val_CER improved from 0.32222 to 0.30784, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.3128 - WER: 0.4180 - loss: 0.4454 - val_CER: 0.3078 - val_WER: 0.4105 - val_loss: 10.4681 - learning_rate: 1.0000e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.3003 - WER: 0.4004 - loss: 0.3656\n",
      "Epoch 19: val_CER improved from 0.30784 to 0.29516, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.2997 - WER: 0.3996 - loss: 0.3727 - val_CER: 0.2952 - val_WER: 0.3927 - val_loss: 13.2144 - learning_rate: 1.0000e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.2880 - WER: 0.3831 - loss: 0.2303\n",
      "Epoch 20: val_CER improved from 0.29516 to 0.28333, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.2874 - WER: 0.3824 - loss: 0.2327 - val_CER: 0.2833 - val_WER: 0.3762 - val_loss: 13.8100 - learning_rate: 1.0000e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.2769 - WER: 0.3678 - loss: 0.1996\n",
      "Epoch 21: val_CER improved from 0.28333 to 0.27226, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.2764 - WER: 0.3671 - loss: 0.1983 - val_CER: 0.2723 - val_WER: 0.3611 - val_loss: 13.0511 - learning_rate: 1.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.2665 - WER: 0.3533 - loss: 0.3939\n",
      "Epoch 22: val_CER improved from 0.27226 to 0.26247, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.2661 - WER: 0.3527 - loss: 0.3831 - val_CER: 0.2625 - val_WER: 0.3474 - val_loss: 13.2591 - learning_rate: 1.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.2565 - WER: 0.3395 - loss: 0.1306\n",
      "Epoch 23: val_CER improved from 0.26247 to 0.25302, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.2562 - WER: 0.3391 - loss: 0.1352 - val_CER: 0.2530 - val_WER: 0.3344 - val_loss: 11.8685 - learning_rate: 1.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.2481 - WER: 0.3277 - loss: 0.4889\n",
      "Epoch 24: val_CER improved from 0.25302 to 0.24530, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.2478 - WER: 0.3273 - loss: 0.5098 - val_CER: 0.2453 - val_WER: 0.3234 - val_loss: 12.8994 - learning_rate: 1.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.2403 - WER: 0.3169 - loss: 0.4475\n",
      "Epoch 25: val_CER improved from 0.24530 to 0.23734, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.2400 - WER: 0.3165 - loss: 0.4353 - val_CER: 0.2373 - val_WER: 0.3127 - val_loss: 9.7278 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.2328 - WER: 0.3068 - loss: 0.2196\n",
      "Epoch 26: val_CER improved from 0.23734 to 0.23044, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.2325 - WER: 0.3064 - loss: 0.2226 - val_CER: 0.2304 - val_WER: 0.3030 - val_loss: 13.5930 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.2263 - WER: 0.2977 - loss: 0.2881\n",
      "Epoch 27: val_CER improved from 0.23044 to 0.22368, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 5s/step - CER: 0.2260 - WER: 0.2973 - loss: 0.2832 - val_CER: 0.2237 - val_WER: 0.2940 - val_loss: 12.7872 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.2201 - WER: 0.2892 - loss: 0.3402\n",
      "Epoch 28: val_CER improved from 0.22368 to 0.21738, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 5s/step - CER: 0.2198 - WER: 0.2889 - loss: 0.3335 - val_CER: 0.2174 - val_WER: 0.2853 - val_loss: 10.0764 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.2139 - WER: 0.2808 - loss: 0.1784\n",
      "Epoch 29: val_CER improved from 0.21738 to 0.21156, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 5s/step - CER: 0.2136 - WER: 0.2805 - loss: 0.1827 - val_CER: 0.2116 - val_WER: 0.2773 - val_loss: 11.3344 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.2081 - WER: 0.2728 - loss: 0.1162\n",
      "Epoch 30: val_CER improved from 0.21156 to 0.20577, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 5s/step - CER: 0.2079 - WER: 0.2725 - loss: 0.1162 - val_CER: 0.2058 - val_WER: 0.2694 - val_loss: 12.7007 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.2026 - WER: 0.2652 - loss: 0.1322\n",
      "Epoch 31: val_CER improved from 0.20577 to 0.20052, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 5s/step - CER: 0.2023 - WER: 0.2649 - loss: 0.1416 - val_CER: 0.2005 - val_WER: 0.2623 - val_loss: 13.8627 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1974 - WER: 0.2582 - loss: 0.0652\n",
      "Epoch 32: val_CER improved from 0.20052 to 0.19531, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 5s/step - CER: 0.1971 - WER: 0.2579 - loss: 0.0653 - val_CER: 0.1953 - val_WER: 0.2552 - val_loss: 11.6293 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1925 - WER: 0.2515 - loss: 0.1219\n",
      "Epoch 33: val_CER improved from 0.19531 to 0.19054, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 5s/step - CER: 0.1923 - WER: 0.2513 - loss: 0.1204 - val_CER: 0.1905 - val_WER: 0.2486 - val_loss: 12.0814 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1877 - WER: 0.2449 - loss: 0.1269\n",
      "Epoch 34: val_CER improved from 0.19054 to 0.18614, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.1875 - WER: 0.2447 - loss: 0.1290 - val_CER: 0.1861 - val_WER: 0.2426 - val_loss: 13.7286 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1837 - WER: 0.2393 - loss: 0.3289\n",
      "Epoch 35: val_CER improved from 0.18614 to 0.18231, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 5s/step - CER: 0.1835 - WER: 0.2391 - loss: 0.3341 - val_CER: 0.1823 - val_WER: 0.2370 - val_loss: 10.9145 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1798 - WER: 0.2339 - loss: 0.0972\n",
      "Epoch 36: val_CER improved from 0.18231 to 0.17834, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1796 - WER: 0.2336 - loss: 0.1008 - val_CER: 0.1783 - val_WER: 0.2318 - val_loss: 12.8102 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1763 - WER: 0.2289 - loss: 0.4015\n",
      "Epoch 37: val_CER improved from 0.17834 to 0.17492, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1761 - WER: 0.2287 - loss: 0.3974 - val_CER: 0.1749 - val_WER: 0.2268 - val_loss: 10.4496 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1727 - WER: 0.2240 - loss: 0.2252\n",
      "Epoch 38: val_CER improved from 0.17492 to 0.17140, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1726 - WER: 0.2238 - loss: 0.2258 - val_CER: 0.1714 - val_WER: 0.2221 - val_loss: 8.8556 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1693 - WER: 0.2194 - loss: 0.0873\n",
      "Epoch 39: val_CER improved from 0.17140 to 0.16796, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1692 - WER: 0.2192 - loss: 0.0894 - val_CER: 0.1680 - val_WER: 0.2173 - val_loss: 10.4465 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1658 - WER: 0.2146 - loss: 0.0787\n",
      "Epoch 40: val_CER improved from 0.16796 to 0.16461, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 5s/step - CER: 0.1657 - WER: 0.2144 - loss: 0.0798 - val_CER: 0.1646 - val_WER: 0.2128 - val_loss: 12.2083 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1626 - WER: 0.2102 - loss: 0.1536\n",
      "Epoch 41: val_CER improved from 0.16461 to 0.16151, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 5s/step - CER: 0.1625 - WER: 0.2100 - loss: 0.1526 - val_CER: 0.1615 - val_WER: 0.2086 - val_loss: 9.8476 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1597 - WER: 0.2062 - loss: 0.1173\n",
      "Epoch 42: val_CER improved from 0.16151 to 0.15860, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 5s/step - CER: 0.1595 - WER: 0.2060 - loss: 0.1185 - val_CER: 0.1586 - val_WER: 0.2045 - val_loss: 13.2022 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1569 - WER: 0.2024 - loss: 0.1759\n",
      "Epoch 43: val_CER improved from 0.15860 to 0.15595, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.1568 - WER: 0.2022 - loss: 0.1780 - val_CER: 0.1560 - val_WER: 0.2009 - val_loss: 14.9807 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1543 - WER: 0.1988 - loss: 0.2814\n",
      "Epoch 44: val_CER improved from 0.15595 to 0.15354, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1542 - WER: 0.1987 - loss: 0.2892 - val_CER: 0.1535 - val_WER: 0.1976 - val_loss: 10.5793 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1520 - WER: 0.1956 - loss: 0.1991\n",
      "Epoch 45: val_CER improved from 0.15354 to 0.15125, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1519 - WER: 0.1954 - loss: 0.2074 - val_CER: 0.1513 - val_WER: 0.1943 - val_loss: 10.5707 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1497 - WER: 0.1923 - loss: 0.1530\n",
      "Epoch 46: val_CER improved from 0.15125 to 0.14887, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.1496 - WER: 0.1922 - loss: 0.1548 - val_CER: 0.1489 - val_WER: 0.1912 - val_loss: 11.4041 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1473 - WER: 0.1892 - loss: 0.1104\n",
      "Epoch 47: val_CER improved from 0.14887 to 0.14651, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.1472 - WER: 0.1890 - loss: 0.1107 - val_CER: 0.1465 - val_WER: 0.1879 - val_loss: 11.0812 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1452 - WER: 0.1862 - loss: 0.2907\n",
      "Epoch 48: val_CER improved from 0.14651 to 0.14427, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 5s/step - CER: 0.1450 - WER: 0.1860 - loss: 0.2795 - val_CER: 0.1443 - val_WER: 0.1848 - val_loss: 9.7472 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1428 - WER: 0.1830 - loss: 0.0458\n",
      "Epoch 49: val_CER improved from 0.14427 to 0.14201, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 5s/step - CER: 0.1427 - WER: 0.1829 - loss: 0.0461 - val_CER: 0.1420 - val_WER: 0.1818 - val_loss: 9.5018 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1406 - WER: 0.1800 - loss: 0.0557\n",
      "Epoch 50: val_CER improved from 0.14201 to 0.13989, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 5s/step - CER: 0.1405 - WER: 0.1799 - loss: 0.0585 - val_CER: 0.1399 - val_WER: 0.1789 - val_loss: 9.8928 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1386 - WER: 0.1772 - loss: 0.0894\n",
      "Epoch 51: val_CER improved from 0.13989 to 0.13792, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 5s/step - CER: 0.1385 - WER: 0.1771 - loss: 0.0894 - val_CER: 0.1379 - val_WER: 0.1761 - val_loss: 13.1382 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1366 - WER: 0.1745 - loss: 0.1007\n",
      "Epoch 52: val_CER improved from 0.13792 to 0.13598, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 5s/step - CER: 0.1366 - WER: 0.1744 - loss: 0.1019 - val_CER: 0.1360 - val_WER: 0.1735 - val_loss: 11.8768 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1349 - WER: 0.1720 - loss: 0.1899\n",
      "Epoch 53: val_CER improved from 0.13598 to 0.13421, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1348 - WER: 0.1719 - loss: 0.1881 - val_CER: 0.1342 - val_WER: 0.1709 - val_loss: 11.2166 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1331 - WER: 0.1695 - loss: 0.0673\n",
      "Epoch 54: val_CER improved from 0.13421 to 0.13238, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1330 - WER: 0.1694 - loss: 0.0676 - val_CER: 0.1324 - val_WER: 0.1685 - val_loss: 11.4107 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1312 - WER: 0.1670 - loss: 0.0670\n",
      "Epoch 55: val_CER improved from 0.13238 to 0.13061, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1311 - WER: 0.1669 - loss: 0.0679 - val_CER: 0.1306 - val_WER: 0.1661 - val_loss: 11.1110 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1295 - WER: 0.1648 - loss: 0.1322\n",
      "Epoch 56: val_CER improved from 0.13061 to 0.12893, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1294 - WER: 0.1646 - loss: 0.1275 - val_CER: 0.1289 - val_WER: 0.1638 - val_loss: 11.4474 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1278 - WER: 0.1624 - loss: 0.0304\n",
      "Epoch 57: val_CER improved from 0.12893 to 0.12724, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1277 - WER: 0.1623 - loss: 0.0303 - val_CER: 0.1272 - val_WER: 0.1615 - val_loss: 11.0666 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1261 - WER: 0.1601 - loss: 0.0281\n",
      "Epoch 58: val_CER improved from 0.12724 to 0.12561, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1261 - WER: 0.1600 - loss: 0.0285 - val_CER: 0.1256 - val_WER: 0.1593 - val_loss: 10.8186 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1246 - WER: 0.1581 - loss: 0.0515\n",
      "Epoch 59: val_CER improved from 0.12561 to 0.12406, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1245 - WER: 0.1580 - loss: 0.0507 - val_CER: 0.1241 - val_WER: 0.1572 - val_loss: 11.8281 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1230 - WER: 0.1559 - loss: 0.0249\n",
      "Epoch 60: val_CER improved from 0.12406 to 0.12254, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1229 - WER: 0.1558 - loss: 0.0249 - val_CER: 0.1225 - val_WER: 0.1552 - val_loss: 11.9200 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1216 - WER: 0.1539 - loss: 0.0286\n",
      "Epoch 61: val_CER improved from 0.12254 to 0.12107, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1215 - WER: 0.1538 - loss: 0.0282 - val_CER: 0.1211 - val_WER: 0.1532 - val_loss: 11.6382 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1201 - WER: 0.1520 - loss: 0.0249\n",
      "Epoch 62: val_CER improved from 0.12107 to 0.11964, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1201 - WER: 0.1519 - loss: 0.0249 - val_CER: 0.1196 - val_WER: 0.1512 - val_loss: 11.3231 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1187 - WER: 0.1500 - loss: 0.0212\n",
      "Epoch 63: val_CER improved from 0.11964 to 0.11826, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1186 - WER: 0.1499 - loss: 0.0212 - val_CER: 0.1183 - val_WER: 0.1493 - val_loss: 11.0792 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1173 - WER: 0.1481 - loss: 0.0203\n",
      "Epoch 64: val_CER improved from 0.11826 to 0.11693, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1172 - WER: 0.1480 - loss: 0.0203 - val_CER: 0.1169 - val_WER: 0.1475 - val_loss: 10.8907 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1160 - WER: 0.1464 - loss: 0.0191\n",
      "Epoch 65: val_CER improved from 0.11693 to 0.11563, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1160 - WER: 0.1463 - loss: 0.0191 - val_CER: 0.1156 - val_WER: 0.1457 - val_loss: 10.7420 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1148 - WER: 0.1447 - loss: 0.0194\n",
      "Epoch 66: val_CER improved from 0.11563 to 0.11438, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1147 - WER: 0.1446 - loss: 0.0194 - val_CER: 0.1144 - val_WER: 0.1440 - val_loss: 10.6353 - learning_rate: 1.0000e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1135 - WER: 0.1430 - loss: 0.0230\n",
      "Epoch 67: val_CER improved from 0.11438 to 0.11316, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1135 - WER: 0.1429 - loss: 0.0228 - val_CER: 0.1132 - val_WER: 0.1424 - val_loss: 10.4850 - learning_rate: 1.0000e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1124 - WER: 0.1414 - loss: 0.0192\n",
      "Epoch 68: val_CER improved from 0.11316 to 0.11197, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.1123 - WER: 0.1413 - loss: 0.0194 - val_CER: 0.1120 - val_WER: 0.1408 - val_loss: 10.2900 - learning_rate: 1.0000e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1112 - WER: 0.1397 - loss: 0.0163\n",
      "Epoch 69: val_CER improved from 0.11197 to 0.11080, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1111 - WER: 0.1397 - loss: 0.0165 - val_CER: 0.1108 - val_WER: 0.1392 - val_loss: 9.7555 - learning_rate: 1.0000e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1100 - WER: 0.1382 - loss: 0.0158\n",
      "Epoch 70: val_CER improved from 0.11080 to 0.10965, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1099 - WER: 0.1381 - loss: 0.0158 - val_CER: 0.1097 - val_WER: 0.1377 - val_loss: 9.7804 - learning_rate: 1.0000e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1089 - WER: 0.1368 - loss: 0.0205\n",
      "Epoch 71: val_CER improved from 0.10965 to 0.10852, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1089 - WER: 0.1367 - loss: 0.0205 - val_CER: 0.1085 - val_WER: 0.1362 - val_loss: 9.1924 - learning_rate: 1.0000e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1078 - WER: 0.1352 - loss: 0.0156\n",
      "Epoch 72: val_CER improved from 0.10852 to 0.10740, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.1077 - WER: 0.1352 - loss: 0.0156 - val_CER: 0.1074 - val_WER: 0.1347 - val_loss: 8.5646 - learning_rate: 1.0000e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.1067 - WER: 0.1338 - loss: 0.0153\n",
      "Epoch 73: val_CER improved from 0.10740 to 0.10631, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.1066 - WER: 0.1337 - loss: 0.0153 - val_CER: 0.1063 - val_WER: 0.1332 - val_loss: 8.4275 - learning_rate: 1.0000e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1056 - WER: 0.1323 - loss: 0.0168\n",
      "Epoch 74: val_CER improved from 0.10631 to 0.10525, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1055 - WER: 0.1322 - loss: 0.0167 - val_CER: 0.1052 - val_WER: 0.1318 - val_loss: 8.8350 - learning_rate: 1.0000e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1045 - WER: 0.1310 - loss: 0.0154\n",
      "Epoch 75: val_CER improved from 0.10525 to 0.10424, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1045 - WER: 0.1309 - loss: 0.0156 - val_CER: 0.1042 - val_WER: 0.1305 - val_loss: 9.2022 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1036 - WER: 0.1296 - loss: 0.0146\n",
      "Epoch 76: val_CER improved from 0.10424 to 0.10326, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1035 - WER: 0.1296 - loss: 0.0146 - val_CER: 0.1033 - val_WER: 0.1292 - val_loss: 9.9089 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1026 - WER: 0.1284 - loss: 0.2705\n",
      "Epoch 77: val_CER improved from 0.10326 to 0.10237, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1026 - WER: 0.1283 - loss: 0.2565 - val_CER: 0.1024 - val_WER: 0.1280 - val_loss: 10.4144 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1017 - WER: 0.1272 - loss: 0.0491\n",
      "Epoch 78: val_CER improved from 0.10237 to 0.10145, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1017 - WER: 0.1272 - loss: 0.0483 - val_CER: 0.1014 - val_WER: 0.1268 - val_loss: 10.2707 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.1008 - WER: 0.1261 - loss: 0.0162\n",
      "Epoch 79: val_CER improved from 0.10145 to 0.10052, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.1008 - WER: 0.1260 - loss: 0.0165 - val_CER: 0.1005 - val_WER: 0.1256 - val_loss: 9.6244 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - CER: 0.0999 - WER: 0.1249 - loss: 0.0157\n",
      "Epoch 80: val_CER improved from 0.10052 to 0.09960, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0999 - WER: 0.1248 - loss: 0.0156 - val_CER: 0.0996 - val_WER: 0.1244 - val_loss: 9.1420 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0990 - WER: 0.1236 - loss: 0.0139\n",
      "Epoch 81: val_CER improved from 0.09960 to 0.09868, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0989 - WER: 0.1236 - loss: 0.0139 - val_CER: 0.0987 - val_WER: 0.1232 - val_loss: 8.9233 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0981 - WER: 0.1225 - loss: 0.0147\n",
      "Epoch 82: val_CER improved from 0.09868 to 0.09778, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0980 - WER: 0.1224 - loss: 0.0147 - val_CER: 0.0978 - val_WER: 0.1221 - val_loss: 8.7913 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0972 - WER: 0.1213 - loss: 0.0134\n",
      "Epoch 83: val_CER improved from 0.09778 to 0.09689, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0971 - WER: 0.1213 - loss: 0.0134 - val_CER: 0.0969 - val_WER: 0.1209 - val_loss: 8.7953 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0963 - WER: 0.1202 - loss: 0.0717\n",
      "Epoch 84: val_CER improved from 0.09689 to 0.09619, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0963 - WER: 0.1202 - loss: 0.0713 - val_CER: 0.0962 - val_WER: 0.1199 - val_loss: 12.6270 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0956 - WER: 0.1192 - loss: 0.0549\n",
      "Epoch 85: val_CER improved from 0.09619 to 0.09542, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0956 - WER: 0.1192 - loss: 0.0571 - val_CER: 0.0954 - val_WER: 0.1189 - val_loss: 7.7651 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0949 - WER: 0.1183 - loss: 0.4296\n",
      "Epoch 86: val_CER improved from 0.09542 to 0.09470, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0949 - WER: 0.1182 - loss: 0.4052 - val_CER: 0.0947 - val_WER: 0.1179 - val_loss: 7.8304 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0942 - WER: 0.1173 - loss: 0.0810\n",
      "Epoch 87: val_CER improved from 0.09470 to 0.09398, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0941 - WER: 0.1172 - loss: 0.0833 - val_CER: 0.0940 - val_WER: 0.1170 - val_loss: 9.9907 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0935 - WER: 0.1163 - loss: 0.0548\n",
      "Epoch 88: val_CER improved from 0.09398 to 0.09325, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0934 - WER: 0.1163 - loss: 0.0552 - val_CER: 0.0932 - val_WER: 0.1160 - val_loss: 7.5804 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0928 - WER: 0.1154 - loss: 0.1369\n",
      "Epoch 89: val_CER improved from 0.09325 to 0.09261, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0928 - WER: 0.1154 - loss: 0.1348 - val_CER: 0.0926 - val_WER: 0.1151 - val_loss: 7.8177 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0921 - WER: 0.1145 - loss: 0.0174\n",
      "Epoch 90: val_CER improved from 0.09261 to 0.09190, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 5s/step - CER: 0.0921 - WER: 0.1144 - loss: 0.0176 - val_CER: 0.0919 - val_WER: 0.1142 - val_loss: 9.4880 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0914 - WER: 0.1136 - loss: 0.0330\n",
      "Epoch 91: val_CER improved from 0.09190 to 0.09122, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0914 - WER: 0.1135 - loss: 0.0359 - val_CER: 0.0912 - val_WER: 0.1133 - val_loss: 6.0854 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0908 - WER: 0.1128 - loss: 0.0916\n",
      "Epoch 92: val_CER improved from 0.09122 to 0.09070, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0908 - WER: 0.1127 - loss: 0.0948 - val_CER: 0.0907 - val_WER: 0.1125 - val_loss: 9.6136 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0902 - WER: 0.1120 - loss: 0.0670\n",
      "Epoch 93: val_CER improved from 0.09070 to 0.09013, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0902 - WER: 0.1119 - loss: 0.0787 - val_CER: 0.0901 - val_WER: 0.1118 - val_loss: 10.5732 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0897 - WER: 0.1112 - loss: 0.1788\n",
      "Epoch 94: val_CER improved from 0.09013 to 0.08950, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0896 - WER: 0.1111 - loss: 0.1783 - val_CER: 0.0895 - val_WER: 0.1109 - val_loss: 4.0021 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0890 - WER: 0.1103 - loss: 0.0314\n",
      "Epoch 95: val_CER improved from 0.08950 to 0.08882, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0890 - WER: 0.1103 - loss: 0.0318 - val_CER: 0.0888 - val_WER: 0.1100 - val_loss: 6.3565 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0884 - WER: 0.1095 - loss: 0.1859\n",
      "Epoch 96: val_CER improved from 0.08882 to 0.08813, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0884 - WER: 0.1095 - loss: 0.1786 - val_CER: 0.0881 - val_WER: 0.1092 - val_loss: 1.8340 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0877 - WER: 0.1086 - loss: 0.0635\n",
      "Epoch 97: val_CER improved from 0.08813 to 0.08745, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0876 - WER: 0.1086 - loss: 0.0617 - val_CER: 0.0875 - val_WER: 0.1083 - val_loss: 3.5292 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0870 - WER: 0.1077 - loss: 0.0152\n",
      "Epoch 98: val_CER improved from 0.08745 to 0.08683, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0870 - WER: 0.1077 - loss: 0.0153 - val_CER: 0.0868 - val_WER: 0.1074 - val_loss: 5.1124 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0864 - WER: 0.1069 - loss: 0.0190\n",
      "Epoch 99: val_CER improved from 0.08683 to 0.08622, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0864 - WER: 0.1069 - loss: 0.0195 - val_CER: 0.0862 - val_WER: 0.1067 - val_loss: 4.8480 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - CER: 0.0858 - WER: 0.1062 - loss: 0.0408\n",
      "Epoch 100: val_CER improved from 0.08622 to 0.08552, saving model to ./saved_models/model.keras\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 5s/step - CER: 0.0858 - WER: 0.1061 - loss: 0.0395 - val_CER: 0.0855 - val_WER: 0.1058 - val_loss: 2.3579 - learning_rate: 1.0000e-04\n",
      "'Functional' object has no attribute '_get_save_spec'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x204b1ec5dc0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Reshape, Conv2D, BatchNormalization, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_resnet_ocr_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Create an OCR model using ResNet50 as the backbone\n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    # Load ResNet50 without top layers\n",
    "    base_model = MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_dim\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the output from ResNet50\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Add custom layers for OCR\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Reshape for sequential processing\n",
    "    _, h, w, c = x.shape\n",
    "    x = Reshape((-1, h * c))(x)\n",
    "    \n",
    "    # Add bidirectional LSTM layers\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = Dense(output_dim + 1, activation='softmax')(x)  # +1 for CTC blank label\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace the original train_model function call with this:\n",
    "model = create_resnet_ocr_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab)\n",
    ")\n",
    "\n",
    "# Fine-tuning setup (add this after initial training)\n",
    "def unfreeze_model(model):\n",
    "    \"\"\"\n",
    "    Unfreeze ResNet50 layers for fine-tuning\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# After initial training, you can fine-tune the model:\n",
    "\n",
    "# Fine-tuning (uncomment and use after initial training)\n",
    "model = unfreeze_model(model)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate * 0.1),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Train for additional epochs with unfrozen layers\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Reshape, Conv2D, BatchNormalization, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_resnet_ocr_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Create an OCR model using ResNet50 as the backbone\n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    # Load ResNet50 without top layers\n",
    "    base_model = Xception(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_dim\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the output from ResNet50\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Add custom layers for OCR\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Reshape for sequential processing\n",
    "    _, h, w, c = x.shape\n",
    "    x = Reshape((-1, h * c))(x)\n",
    "    \n",
    "    # Add bidirectional LSTM layers\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = Dense(output_dim + 1, activation='softmax')(x)  # +1 for CTC blank label\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace the original train_model function call with this:\n",
    "model = create_resnet_ocr_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab)\n",
    ")\n",
    "\n",
    "# Fine-tuning setup (add this after initial training)\n",
    "def unfreeze_model(model):\n",
    "    \"\"\"\n",
    "    Unfreeze ResNet50 layers for fine-tuning\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# After initial training, you can fine-tune the model:\n",
    "\n",
    "# Fine-tuning (uncomment and use after initial training)\n",
    "model = unfreeze_model(model)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate * 0.1),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Train for additional epochs with unfrozen layers\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Reshape, Conv2D, BatchNormalization, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_resnet_ocr_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Create an OCR model using ResNet50 as the backbone\n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    # Load ResNet50 without top layers\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_dim\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the output from ResNet50\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Add custom layers for OCR\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Reshape for sequential processing\n",
    "    _, h, w, c = x.shape\n",
    "    x = Reshape((-1, h * c))(x)\n",
    "    \n",
    "    # Add bidirectional LSTM layers\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = Dense(output_dim + 1, activation='softmax')(x)  # +1 for CTC blank label\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace the original train_model function call with this:\n",
    "model = create_resnet_ocr_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab)\n",
    ")\n",
    "\n",
    "# Fine-tuning setup (add this after initial training)\n",
    "def unfreeze_model(model):\n",
    "    \"\"\"\n",
    "    Unfreeze ResNet50 layers for fine-tuning\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# After initial training, you can fine-tune the model:\n",
    "\n",
    "# Fine-tuning (uncomment and use after initial training)\n",
    "model = unfreeze_model(model)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate * 0.1),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Train for additional epochs with unfrozen layers\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Reshape, Conv2D, BatchNormalization, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_resnet_ocr_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Create an OCR model using ResNet50 as the backbone\n",
    "    Args:\n",
    "        input_dim: Tuple of (height, width, channels)\n",
    "        output_dim: Number of classes (vocabulary size)\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled model\n",
    "    \"\"\"\n",
    "    # Load ResNet50 without top layers\n",
    "    base_model = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_dim\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Get the output from ResNet50\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Add custom layers for OCR\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Reshape for sequential processing\n",
    "    _, h, w, c = x.shape\n",
    "    x = Reshape((-1, h * c))(x)\n",
    "    \n",
    "    # Add bidirectional LSTM layers\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    outputs = Dense(output_dim + 1, activation='softmax')(x)  # +1 for CTC blank label\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Replace the original train_model function call with this:\n",
    "model = create_resnet_ocr_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab)\n",
    ")\n",
    "\n",
    "# Fine-tuning setup (add this after initial training)\n",
    "def unfreeze_model(model):\n",
    "    \"\"\"\n",
    "    Unfreeze ResNet50 layers for fine-tuning\n",
    "    \"\"\"\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model\n",
    "\n",
    "# After initial training, you can fine-tune the model:\n",
    "\n",
    "# Fine-tuning (uncomment and use after initial training)\n",
    "model = unfreeze_model(model)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate * 0.1),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "# Train for additional epochs with unfrozen layers\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imporved ocr code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
